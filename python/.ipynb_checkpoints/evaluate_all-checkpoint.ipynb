{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import ntpath\n",
    "import os\n",
    "from evaluate import calc_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../settings.txt') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "GT_IDS = settings['individual_gt_ids']\n",
    "REPRESENTATIVE_IOU = settings['representative_iou']\n",
    "OUT_DIR = settings['output_base_path'] + '/' + settings['evaluation_result_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def aggregate_scores(filter_func=lambda x: True, suffix='stats'):\n",
    "    \n",
    "    with open('../file_params.txt') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    datasets = []\n",
    "    magnifications = []\n",
    "    precision_each = []\n",
    "    recall_each = []\n",
    "    f1_each = []\n",
    "    first = True\n",
    "    for key, param in params.items():\n",
    "        if(not filter_func(param)):\n",
    "            continue\n",
    "        eval_dir = settings['output_base_path'] + '/' + param['output_path']\n",
    "        eval_dir += '/' + settings['evaluation_result_path']\n",
    "        basename, _ = os.path.splitext(path_leaf(param['filename']))\n",
    "        print(eval_dir, basename)\n",
    "        df = pd.read_csv(eval_dir + '/' + basename + '_' + suffix + '.csv')\n",
    "        if(first):\n",
    "            df_sum = df[['TruePos', 'FalsePos', 'FalseNeg']]\n",
    "            thresholds = df['IoU_Thresh']\n",
    "            indices = np.where(thresholds >= REPRESENTATIVE_IOU)\n",
    "            representative_iou_index = indices[0][0]\n",
    "            first = False\n",
    "        else:\n",
    "            df_sum += df\n",
    "        datasets.append(key)\n",
    "        magnifications.append(param['magnification'])\n",
    "        precision_each.append(df['Precision'][representative_iou_index])\n",
    "        recall_each.append(df['Recall'][representative_iou_index])\n",
    "        f1_each.append(df['F1'][representative_iou_index])\n",
    "\n",
    "    f1_all, precision_all, recall_all = calc_f1_scores(df_sum)\n",
    "    \n",
    "    f1_rep = f1_all[representative_iou_index]\n",
    "\n",
    "    df_sum.insert(0, 'IoU_Thresh', thresholds)\n",
    "    df_sum['Precision'] = precision_all\n",
    "    df_sum['Recall'] = recall_all\n",
    "    df_sum['F1'] = f1_all\n",
    "    \n",
    "    df_each = pd.DataFrame(datasets, columns=['Dataset'])\n",
    "    df_each['Magnification'] = magnifications\n",
    "    df_each['Precision'] = precision_each\n",
    "    df_each['Recall'] = recall_each\n",
    "    df_each['F1'] = f1_each\n",
    "    \n",
    "    return f1_rep, df_sum, df_each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ramdas/Documents/Voltage_Imaging/Output//evaluated'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ramdas/Documents/Voltage_Imaging/Output///home/ramdas/Documents/Voltage_Imaging/Output//evaluated/000_stats.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-27c29b33ed88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_each\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/all_stats.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_each\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/each_stats.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IoU_Thresh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7e601a5a5951>\u001b[0m in \u001b[0;36maggregate_scores\u001b[0;34m(filter_func, suffix)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0meval_dir\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evaluation_result_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbasename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdf_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TruePos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FalsePos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FalseNeg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltage/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltage/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltage/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltage/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/voltage/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ramdas/Documents/Voltage_Imaging/Output///home/ramdas/Documents/Voltage_Imaging/Output//evaluated/000_stats.csv'"
     ]
    }
   ],
   "source": [
    "f1_rep, df_sum, df_each = aggregate_scores()\n",
    "df_sum.to_csv(OUT_DIR + '/all_stats.csv', index=False)\n",
    "df_each.to_csv(OUT_DIR + '/each_stats.csv', index=False)\n",
    "thresholds = df_sum['IoU_Thresh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "display(Markdown('# F1 = %.2f' % f1_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION_THRESH = 20\n",
    "f1_rep_16x, df_sum_16x, df_each_16x = aggregate_scores(lambda x: x['magnification'] <= MAGNIFICATION_THRESH)\n",
    "f1_rep_40x, df_sum_40x, df_each_40x = aggregate_scores(lambda x: x['magnification'] > MAGNIFICATION_THRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_score(df, rep, title):\n",
    "    plt.axis('square')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(thresholds, df['Precision'], label='Precision')\n",
    "    plt.plot(thresholds, df['Recall'], label='Recall')\n",
    "    plt.plot(thresholds, df['F1'], label='F1 score')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('IoU Threshold')\n",
    "    plt.vlines(REPRESENTATIVE_IOU, 0, 1, colors='gray', linestyles='dashed')\n",
    "    plt.hlines(rep, REPRESENTATIVE_IOU - 0.1, REPRESENTATIVE_IOU + 0.1, colors='gray', linestyles='dashed')\n",
    "    plt.text(REPRESENTATIVE_IOU + 0.1, rep, 'F1 = %.2f' % rep)\n",
    "    plt.title(title)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.suptitle('Accuracy Summary', fontsize=16)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_f1_score(df_sum, f1_rep, 'All datasets')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_f1_score(df_sum_16x, f1_rep_16x, '16x datasets')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_f1_score(df_sum_40x, f1_rep_40x, '40x datasets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_dataset_score(df, column, label, color):\n",
    "    scores = df[column]\n",
    "    keys = df['Dataset']\n",
    "    mags = df['Magnification']\n",
    "    plt.figure(figsize=(17, 3))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y')\n",
    "    plt.bar(list(range(len(scores))), scores, color=color) \n",
    "    plt.xticks(list(range(len(scores))), keys, rotation='vertical')\n",
    "    for mag, ticklabel in zip(mags, plt.gca().get_xticklabels()):\n",
    "        if(mag >= 40):\n",
    "            ticklabel.set_color('green')\n",
    "        elif(mag >= 20):\n",
    "            ticklabel.set_color('blue')\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('Dataset  (black 16x, blue 20x, green 40x)')\n",
    "    plt.title('Per-dataset ' + label + ' at IoU = %.1f' % REPRESENTATIVE_IOU)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_dataset_score(df_each, 'F1', 'F1 score', 'C2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_dataset_score(df_each, 'Precision', 'Precision', 'C0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_dataset_score(df_each, 'Recall', 'Recall', 'C1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_reps = {}\n",
    "scores = {}\n",
    "for gt_id in GT_IDS:\n",
    "    f1_rep, df_sum, _ = aggregate_scores(suffix=gt_id)\n",
    "    f1_reps[gt_id] = f1_rep\n",
    "    scores[gt_id] = df_sum\n",
    "    \n",
    "def plot_score_multiple_gt(scores, idx, label, reps=None):\n",
    "    plt.axis('square')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    for i, gt_id in enumerate(GT_IDS):\n",
    "        plt.plot(thresholds, scores[gt_id][idx], label=gt_id)\n",
    "        if(reps != None):\n",
    "            rep = reps[gt_id]\n",
    "            plt.text(REPRESENTATIVE_IOU + 0.1, rep, 'F1 = %.2f' % rep, color='C%d' % i)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('IoU Threshold')\n",
    "    plt.vlines(REPRESENTATIVE_IOU, 0, 1, colors='gray', linestyles='dashed')\n",
    "    plt.title(label)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.suptitle('Accuracy for Individual GTs', fontsize=16)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_score_multiple_gt(scores, 'F1', 'F1 score', f1_reps)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_score_multiple_gt(scores, 'Precision', 'Precision')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_score_multiple_gt(scores, 'Recall', 'Recall')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
