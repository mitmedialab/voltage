{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import ntpath\n",
    "import os\n",
    "from evaluate import calc_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('settings.txt') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "GT_IDS = settings['individual_gt_ids']\n",
    "REPRESENTATIVE_IOU = settings['representative_iou']\n",
    "OUT_DIR = settings['output_base_path'] + '/' + settings['evaluation_result_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def aggregate_scores(filter_func=lambda x: True):\n",
    "    \n",
    "    with open('file_params.txt') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    datasets = []\n",
    "    magnifications = []\n",
    "    precision_each = []\n",
    "    recall_each = []\n",
    "    f1_each = []\n",
    "    first = True\n",
    "    for key, param in params.items():\n",
    "        if(not filter_func(param)):\n",
    "            continue\n",
    "        eval_dir = settings['output_base_path'] + '/' + param['output_path']\n",
    "        eval_dir += '/' + settings['evaluation_result_path']\n",
    "        basename, _ = os.path.splitext(path_leaf(param['filename']))\n",
    "        df = pd.read_csv(eval_dir + '/' + basename + '_stats.csv')\n",
    "        if(first):\n",
    "            df_sum = df[['TruePos', 'FalsePos', 'FalseNeg']]\n",
    "            thresholds = df['IoU_Thresh']\n",
    "            indices = np.where(thresholds >= REPRESENTATIVE_IOU)\n",
    "            representative_iou_index = indices[0][0]\n",
    "            first = False\n",
    "        else:\n",
    "            df_sum += df\n",
    "        datasets.append(key)\n",
    "        magnifications.append(param['magnification'])\n",
    "        precision_each.append(df['Precision'][representative_iou_index])\n",
    "        recall_each.append(df['Recall'][representative_iou_index])\n",
    "        f1_each.append(df['F1'][representative_iou_index])\n",
    "\n",
    "    f1_all, precision_all, recall_all = calc_f1_scores(df_sum)\n",
    "    \n",
    "    f1_rep = f1_all[representative_iou_index]\n",
    "\n",
    "    df_sum.insert(0, 'IoU_Thresh', thresholds)\n",
    "    df_sum['Precision'] = precision_all\n",
    "    df_sum['Recall'] = recall_all\n",
    "    df_sum['F1'] = f1_all\n",
    "    \n",
    "    df_each = pd.DataFrame(datasets, columns=['Dataset'])\n",
    "    df_each['Magnification'] = magnifications\n",
    "    df_each['Precision'] = precision_each\n",
    "    df_each['Recall'] = recall_each\n",
    "    df_each['F1'] = f1_each\n",
    "    \n",
    "    return f1_rep, df_sum, df_each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rep, df_sum, df_each = aggregate_scores()\n",
    "df_sum.to_csv(OUT_DIR + '/all_stats.csv', index=False)\n",
    "df_each.to_csv(OUT_DIR + '/each_stats.csv', index=False)\n",
    "thresholds = df_sum['IoU_Thresh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "display(Markdown('# F1 = %.2f' % f1_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION_THRESH = 20\n",
    "f1_rep_16x, df_sum_16x, df_each_16x = aggregate_scores(lambda x: x['magnification'] <= MAGNIFICATION_THRESH)\n",
    "f1_rep_40x, df_sum_40x, df_each_40x = aggregate_scores(lambda x: x['magnification'] > MAGNIFICATION_THRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_score(df, rep, title):\n",
    "    plt.axis('square')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(thresholds, df['Precision'], label='Precision')\n",
    "    plt.plot(thresholds, df['Recall'], label='Recall')\n",
    "    plt.plot(thresholds, df['F1'], label='F1 score')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('IoU Threshold')\n",
    "    plt.vlines(REPRESENTATIVE_IOU, 0, 1, colors='gray', linestyles='dashed')\n",
    "    plt.hlines(rep, REPRESENTATIVE_IOU - 0.1, REPRESENTATIVE_IOU + 0.1, colors='gray', linestyles='dashed')\n",
    "    plt.text(REPRESENTATIVE_IOU + 0.1, rep, 'F1 = %.2f' % rep)\n",
    "    plt.title(title)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.suptitle('Accuracy Summary', fontsize=16)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_f1_score(df_sum, f1_rep, 'All datasets')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_f1_score(df_sum_16x, f1_rep_16x, '16x datasets')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_f1_score(df_sum_40x, f1_rep_40x, '40x datasets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_dataset_score(df, column, label, color):\n",
    "    scores = df[column]\n",
    "    keys = df['Dataset']\n",
    "    mags = df['Magnification']\n",
    "    plt.figure(figsize=(17, 3))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y')\n",
    "    plt.bar(list(range(len(scores))), scores, color=color) \n",
    "    plt.xticks(list(range(len(scores))), keys, rotation='vertical')\n",
    "    for mag, ticklabel in zip(mags, plt.gca().get_xticklabels()):\n",
    "        if(mag >= 40):\n",
    "            ticklabel.set_color('green')\n",
    "        elif(mag >= 20):\n",
    "            ticklabel.set_color('blue')\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('Dataset  (black 16x, blue 20x, green 40x)')\n",
    "    plt.title('Per-dataset ' + label + ' at IoU = %.1f' % REPRESENTATIVE_IOU)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_dataset_score(df_each, 'F1', 'F1 score', 'C2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_dataset_score(df_each, 'Precision', 'Precision', 'C0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_dataset_score(df_each, 'Recall', 'Recall', 'C1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
