{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpy\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "from evaluate import read_roi, count_matches, calc_f1_scores\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook evaluates individual annotations against \"ground truth (GT)\" masks.\n",
    "# GT is contructed through a consensus building process where individual annotators\n",
    "# look at majority voting results (see majority_vote.ipynb) and resolve any discrepancy\n",
    "# through discussion and redrawing ROIs.\n",
    "# Evaluation here shows how individual judgement varies with respect to the consensus,\n",
    "# providing a target accuracy. If an automatic segmentation method achieves the target,\n",
    "# it can be considered that it is on par with individual human judgement.\n",
    "\n",
    "# GT directory\n",
    "# Consensus GT should be placed directly under it, and individual annotations should\n",
    "# be placed in subdirectories.\n",
    "paths = runpy.run_path('../params/paths.py')\n",
    "GT_DIR = Path(paths['HPC2_DATASETS'], 'HPC2_GT')\n",
    "\n",
    "# Representative IoU threshold\n",
    "params = runpy.run_path('../params/defaults.py')\n",
    "REPRESENTATIVE_IOU = params['REPRESENTATIVE_IOU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gt(gt_dir, thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate individual annotations against ground truth masks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_dir : string\n",
    "        Directory path to GT labelings. Consensus labels must be placed\n",
    "        directly under this directory whereas individual labels must be\n",
    "        placed under its subdirectories.\n",
    "    thresholds : list of float\n",
    "        IoU thresholds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f1_dict : dictionary\n",
    "        Dictionary containing F1 scores for the individual annotations.\n",
    "\n",
    "    \"\"\"\n",
    "    EXT_LIST = ['.tif', '.roi', '.zip']\n",
    "\n",
    "    gt_dir = Path(gt_dir)\n",
    "    gt_files = sorted(gt_dir.glob('*.tif'))\n",
    "    gt_subdirs = sorted([x for x in gt_dir.iterdir() if x.is_dir()])\n",
    "\n",
    "    f1_dict = {}\n",
    "    f1_sum = np.zeros(len(thresholds))\n",
    "    for subdir in gt_subdirs: # iterate over individual labels\n",
    "        \n",
    "        sum_counts = np.zeros((len(thresholds), 3), dtype=int)\n",
    "        for gt_file in gt_files:\n",
    "            gt_masks = tiff.imread(gt_file).astype(bool)\n",
    "            eval_masks = np.zeros((0,) + gt_masks.shape[1:])\n",
    "            for ext in EXT_LIST:\n",
    "                eval_file = subdir.joinpath(gt_file.stem + ext)\n",
    "                if(eval_file.exists()):\n",
    "                    eval_masks = read_roi(eval_file, gt_masks.shape[1:])\n",
    "                    break\n",
    "\n",
    "            counts, _ = count_matches(eval_masks, gt_masks, thresholds)\n",
    "            sum_counts += np.array(counts)\n",
    "\n",
    "        f1, _, _ = calc_f1_scores(sum_counts)\n",
    "        annotator = subdir.name\n",
    "        f1_dict[annotator] = f1\n",
    "        f1_sum += f1\n",
    "\n",
    "    f1_dict['mean'] = f1_sum / len(gt_subdirs)\n",
    "    return f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_F1s(f1_dict, thresholds, representative_iou):\n",
    "    \"\"\"\n",
    "    Plot multiple F1 score curves.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f1_dict : dictionary of list of float\n",
    "        Multiple sets of F1 scores. Each item in the dictionary is a pair of\n",
    "        a label name and a list of F1 scores for varying IoU thresholds.\n",
    "    thresholds : list of float\n",
    "        IoU thresholds.\n",
    "    representative_iou : float\n",
    "        IoU threshold at which representative F1 scores will be reported.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('square')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    offset = 0.005 # so points on y=1 will be visible\n",
    "\n",
    "    plt.plot(thresholds, f1_dict['mean'] - offset, label='mean')\n",
    "    for label, f1 in f1_dict.items():\n",
    "        if(label != 'mean'):\n",
    "            plt.plot(thresholds, f1 - offset, label=label)\n",
    "\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('IoU Threshold')\n",
    "    plt.vlines(representative_iou, 0, 1, colors='gray', linestyles='dashed') \n",
    "\n",
    "    indices = np.where(thresholds >= representative_iou)\n",
    "    rep_idx = indices[0][0]\n",
    "    title = 'F1 = %.2f (mean)' % f1_dict['mean'][rep_idx]\n",
    "    for label, f1 in f1_dict.items():\n",
    "        if(label != 'mean'):\n",
    "            title += ', %.2f (%s)' % (f1[rep_idx], label)\n",
    "    plt.title(title + ' at IoU = %.1f' % representative_iou)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multiple_F1s(f1_dict, thresholds):\n",
    "    \"\"\"\n",
    "    Save multiple F1 score curves in text file for plotting outside of this notebook.\n",
    "    The parameters are the same as those in plot_multiple_F1s().\n",
    "    \"\"\"\n",
    "    with open('f1s.dat', 'w') as f:\n",
    "        for i, t in enumerate(thresholds):\n",
    "            f1s = [t, f1_dict['mean'][i]]\n",
    "            for label, f1 in f1_dict.items():\n",
    "                if(label != 'mean'):\n",
    "                    f1s.append(f1[i])\n",
    "            f.write(' '.join(str(v) for v in f1s) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THRESHOLDS = 100\n",
    "thresholds = np.array(range(1, NUM_THRESHOLDS+1)) / NUM_THRESHOLDS\n",
    "f1_dict = evaluate_gt(GT_DIR, thresholds)\n",
    "save_multiple_F1s(f1_dict, thresholds)\n",
    "plot_multiple_F1s(f1_dict, thresholds, REPRESENTATIVE_IOU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
